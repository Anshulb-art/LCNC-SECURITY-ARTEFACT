# LCNC Security Artefact — README

This repository documents a reproducible security assessment of three default Mendix applications using:

* **SAST** (Static Application Security Testing) with SonarQube
* **DAST** (Dynamic Application Security Testing) with OWASP ZAP (baseline + manual active checks)
* **BugBug** (record‑and‑replay end‑to‑end functional flows, used as evidence for RBAC/IDOR checks)

The goal is to provide **clear, clear** steps so anoyone can run the same tests and obtain the same evidence. The repo is organized by **tool** and by **application**, and every test produces a concrete artifact (report or screenshot) saved under `reports/`.

> **Scope & principle.** Tests run on **local developer instances** of the Mendix sample apps, with **no app model changes** beyond what’s necessary to boot the app. Security level: **Demo/Prototype** (unless stated otherwise). No production systems are scanned.


## 1) Repository structure

> If your local folder names differ, adapt the paths below. The structure is intentionally simple and mirrors how the tests are executed and how evidence is stored.

<repo-root>/
  apps/
    CoffeeService/
    PurchaseRequest/
    TaskTracker/
  tests/
    SAST/
      CoffeeService/
      PurchaseRequest/
      TaskTracker/
    DAST/
      CoffeeService/
      PurchaseRequest/
      TaskTracker/
    BugBug/
      CoffeeService/
      PurchaseRequest/
      TaskTracker/
  reports/
    CoffeeService/
      SAST/
      DAST/
      BugBug/
    PurchaseRequest/
      SAST/
      DAST/
      BugBug/
    TaskTracker/
      SAST/
      DAST/
      BugBug/
  docs/
    structure-folders.txt          # optional: generated by `tree`
    counts.txt                     # optional: folder/file counts
  sonar-project.properties         # if SAST is run from repo root
  .env.example                     # optional: example variables (no secrets)
  README.md                        # this file
```
**Naming convention for evidence** (files in `reports/…`):

```
YYYYMMDD_app_tool_topic.ext
# examples
20250903_TaskTracker_DAST_zap-baseline.html
20250903_PurchaseRequest_BugBug_RBAC-approver-flow.png
20250903_CoffeeService_SAST_sonar-summary.txt
```


## 2) Prerequisites

* Windows 10/11 with **PowerShell**
* **Mendix Studio Pro** (version matching the sample apps)
* Optional but recommended: **Docker Desktop** (for SonarQube and ZAP containers)
* Browser (Chrome/Edge) for manual checks and BugBug recording
* **Do not** commit secrets. Use environment variables while running.

## 3) Quick start (10‑minute overview)

1. **Run one app locally** in Mendix Studio Pro (e.g., TaskTracker → *Run Locally*). Note the URL (e.g., `http://localhost:8082/`).
2. **SAST**: Analyze the app’s project directory with SonarQube. Save the summary to `reports/<App>/SAST/`.
3. **DAST (Baseline)**: Run OWASP ZAP baseline scan against the public pages (e.g., `/index.html`). Save `zap-baseline-<app>.html` under `reports/<App>/DAST/`.
4. **Manual Security Checks**: Log in (demo users), attempt RBAC negative tests and simple IDOR checks, capture **BugBug** recordings and screenshots to `reports/<App>/BugBug/`.
5. **Write down findings** (1–2 paragraphs per app) and map them to mitigations. Keep it concise, factual, and tied to evidence files.


## 4) Running the apps locally

* Open each app from `apps/<AppName>/` in Mendix Studio Pro.
* Use **Run Locally**. Typical URLs: `http://localhost:8081/`, `http://localhost:8082/`, etc.
* Use the **built‑in demo users** (e.g., `MxAdmin/1` or role‑based demo users) when available. If credentials differ in your app, document them inside `tests/<TOOL>/<App>/README.md`.

> If switching to **Production** security causes entity access errors, keep **Demo/Prototype**. The purpose here is to **observe baseline risks** without changing the model.


## 5) SAST (SonarQube)

### 5.1 Setup SonarQube with Docker (recommended)

```powershell
# Start SonarQube locally
docker run -d --name sonarqube -p 9000:9000 sonarqube:lts-community
# Wait ~1–2 minutes, then open http://localhost:9000
# Create a token in SonarQube UI (Administration → Security → Users → Tokens)
```

### 5.2 Run the analysis

**Option A — from the app folder using sonar-scanner CLI**

```powershell
# Example: analyzing TaskTracker sources
Set-Location "<repo-root>\apps\TaskTracker"
# Set env var once in this shell (replace with your token)
$env:SONAR_HOST_URL = "http://localhost:9000"
$env:SONAR_TOKEN    = "<YOUR_TOKEN>"

# Provide a minimal sonar-project.properties next to the project
# (sample provided at repo root — copy and adjust paths if needed)
sonar-scanner
```

**Option B — Dockerized scanner from repo root**

```powershell
Set-Location "<repo-root>"
$env:SONAR_HOST_URL = "http://host.docker.internal:9000"
$env:SONAR_TOKEN    = "<YOUR_TOKEN>"

docker run --rm ^
  -e SONAR_HOST_URL=$env:SONAR_HOST_URL ^
  -e SONAR_LOGIN=$env:SONAR_TOKEN ^
  -v "$PWD:/usr/src" ^
  sonarsource/sonar-scanner-cli
```

### 5.3 What to capture as evidence

* A short text summary exported to `reports/<App>/SAST/` (e.g., key issues, hotspots)
* Screenshot of the SonarQube **Issues** and **Security Hotspots** tabs for the app
* Optional: `sonar-report.txt` with the scanner console output

> Keep SAST commentary tight: list the top 3–5 findings that a Mendix developer can actually act on (e.g., hardcoded credentials, insecure HTTP endpoints in constants, missing secrets handling).


## 6) DAST (OWASP ZAP)

### 6.1 Baseline scan (unauthenticated)

> Baseline establishes the “outside-in” posture with no login. It catches many obvious issues (missing security headers, mixed content, etc.).

```powershell
# Example for TaskTracker running at http://localhost:8082
# Save the report into the app’s reports folder
$reports = "<repo-root>\reports\TaskTracker"
mkdir $reports\DAST -Force | Out-Null

docker run --rm ^
  -v "$reports\DAST:/zap/wrk" ^
  ghcr.io/zaproxy/zaproxy:stable ^
  zap-baseline.py -t http://host.docker.internal:8082/index.html ^
                  -r zap-baseline-tasktracker.html ^
                  -m 5
```

### 6.2 Manual active checks (focused, screenshot‑driven)

> No scripted auth is required here. Use the running app in your browser, log in with demo users, and manually probe for authorization problems. Save **each** check as a screenshot or short screen recording.

**RBAC negative tests** (attempt actions with the wrong role):

* Try creating/approving a request with a user that should **not** have that permission.
* Expect a clear denial. If action succeeds, capture evidence and note the URL/action.

**IDOR sanity checks** (numeric ID tweaks only):

* After logging in, open a record you own (e.g., `.../item/123`). Change the ID in the URL to another value.
* If the app shows someone else’s data to you, capture a screenshot and the exact URL/steps.

**API visibility** (if REST endpoints exist):

* Try `http://localhost:808X/rest-doc/` or `.../openapi.json` if present.
* Use Postman to call a **safe** GET endpoint without auth; confirm 401/403.

Save all evidence to `reports/<App>/DAST/` with the naming convention.

> Keep manual checks realistic. We are not performing destructive tests or fuzzing. The intent is to verify **role boundaries** and **object ownership**.


## 7) BugBug (E2E flows as evidence)

BugBug is used to **record real user flows** that illustrate permissions working (or failing). These recordings double as reproducible proof for RBAC/IDOR claims.

**Typical flows to record per app:**

* **Requester flow**: login → create item → view own item → attempt restricted action
* **Approver/Admin flow**: login → list pending → approve/reject → verify audit/log (if visible)

**What to export**

* Export screenshots (or short videos) of the passing/blocked steps
* Save under `reports/<App>/BugBug/`
* For each flow, add a short `NOTES.txt` with: test goal, steps, expected vs. actual outcome

> If BugBug cloud is used, avoid exporting sensitive data. Redact where needed. The goal is evidencing **authorization behavior**, not performance.



## 8) Findings format (per app)

Create a short `SUMMARY.md` under each app’s `reports/<App>/` describing:

* **Context**: app name, date, URL used (localhost port)
* **SAST highlights**: 3–5 items, link to evidence files
* **DAST highlights**: baseline headline and any manual RBAC/IDOR/API observations
* **BugBug evidence**: list flows recorded and what they demonstrate
* **Mitigations**: 3–5 practical actions (e.g., enable auth for endpoint X, tighten entity access, remove hardcoded secrets)

**Tone**: plain language, no filler. Focus on what a Mendix developer changes on Monday morning.



## 9) Methodology & evaluation

* **Design Science Research (DSR)** framing: artefact = this repo + repeatable steps + captured evidence.
* **Evaluation metrics** (per app):

  * number of SAST issues identified (by severity),
  * number of DAST observations (baseline + manual),
  * number of RBAC/IDOR test cases demonstrated via BugBug,
  * clarity/completeness of evidence (are steps and files sufficient for reproduction?).

Keep all counts in a small table at `reports/metrics.csv` (optional but recommended).



## 10) Troubleshooting (short and practical)

**Docker can’t connect / named pipe error**

* Start **Docker Desktop**, then retry. On Windows, use `host.docker.internal` from containers to reach `localhost` apps.

**ZAP report is empty or times out**

* Confirm the app is running and reachable in a browser.
* Increase `-m` minutes. Scan a specific page (e.g., `/index.html`) instead of the root.

**SonarQube scanner fails auth**

* Regenerate a user token in SonarQube. Avoid admin/admin defaults.
* Verify `SONAR_HOST_URL` and `SONAR_LOGIN`/`SONAR_TOKEN` env vars.

**Entity access warnings in Production mode**

* Stay on **Demo/Prototype** for this artefact. Document the constraint.



## 11) Housekeeping

* Keep raw evidence files under `reports/…` only. Do not scatter in test folders.
* Do not commit secrets. Use `ENV` vars and provide `.env.example` with placeholders.
* If you generate `docs/structure-folders.txt` and `docs/counts.txt` using `tree`, commit them so reviewers can navigate faster.

**PowerShell helpers (optional):**

```powershell
# Generate a simple structure and counts for docs
$root = "<repo-root>"
mkdir "$root\docs" -Force | Out-Null

tree /A "$root" > "$root\docs\structure-folders.txt"

$totalFolders = (Get-ChildItem $root -Directory -Recurse | Measure-Object).Count
$totalFiles   = (Get-ChildItem $root -File -Recurse | Measure-Object).Count
"Total folders: $totalFolders`nTotal files: $totalFiles" | Set-Content "$root\docs\counts.txt"
```



## 12) Submission checklist (what a reviewer needs)

* [ ] This README at repo root (clean and current)
* [ ] Apps run locally; ports and demo users documented
* [ ] SAST reports/screenshots for all three apps
* [ ] ZAP baseline HTML for each app + manual RBAC/IDOR screenshots
* [ ] BugBug flow evidence and short notes per flow
* [ ] Concise `SUMMARY.md` per app with findings and mitigations
* [ ] Optional metrics table (`reports/metrics.csv`)



## 13) Credits & license

* Tools: SonarQube, OWASP ZAP, BugBug
* Apps: Mendix sample applications (used for educational research)
* License: educational use. Replace with your preferred license if needed.


### Appendix — Minimal `sonar-project.properties` (example)

```
sonar.projectKey=TaskTracker
sonar.projectName=TaskTracker
sonar.sources=.
sonar.exclusions=**/deployment/**,**/resources/**,**/*.mpk,**/.mendix-cache/**
```

> Duplicate this file for each app, or parameterize the key and name when running the scanner.
